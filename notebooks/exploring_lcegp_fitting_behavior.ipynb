{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "LCEGP shows strange behavior with the PCS. In theory, adding more training samples\n",
    "should make the model more accurate, and lead to increased PCS. Although this does not\n",
    "translate to the Bayesian PCS we use directly, we would still expect to see similar\n",
    "behavior. This does not always happen with LCEGP, which makes me suspect that it may\n",
    "have to do with the model fitting.\n",
    "\n",
    "The goal of this notebook is to analyze the stability of fitting LCEGP using\n",
    "`fit_gpytorch_model`, and if found to not be stable, come up with better ways of\n",
    "fitting these models.\n",
    "\n",
    "One important question is to understand the effects of the initial embedding on the\n",
    "resulting fitted embedding. I suspect the MLL to be very non-convext in the embedding\n",
    "and this being the possible reason why we observe these inconsistencies.\n",
    "\n",
    "### Implemented `custom_fit_gpytorch_model` to get around this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from botorch import fit_gpytorch_model\n",
    "from gpytorch import ExactMarginalLogLikelihood\n",
    "from torch import Tensor\n",
    "\n",
    "from contextual_rs.lce_gp import LCEGP\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's train several models on same data and see how the results compare.\n",
    "Starting with the purely categorical setting."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saitcakmak/anaconda3/envs/contextual_rs/lib/python3.8/site-packages/gpytorch/models/exact_gp.py:273: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n",
      "/home/saitcakmak/anaconda3/envs/contextual_rs/lib/python3.8/site-packages/gpytorch/models/exact_gp.py:273: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n",
      "/home/saitcakmak/anaconda3/envs/contextual_rs/lib/python3.8/site-packages/gpytorch/models/exact_gp.py:273: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n",
      "/home/saitcakmak/anaconda3/envs/contextual_rs/lib/python3.8/site-packages/gpytorch/models/exact_gp.py:273: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n",
      "/home/saitcakmak/anaconda3/envs/contextual_rs/lib/python3.8/site-packages/gpytorch/models/exact_gp.py:273: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def test_function(X: Tensor) -> Tensor:\n",
    "    sine = torch.sin(X)\n",
    "    linear = X * 0.05\n",
    "    noise = torch.randn_like(X) * 0.25\n",
    "    return sine + linear + noise\n",
    "\n",
    "\n",
    "num_alternatives = 5\n",
    "num_train = 10\n",
    "train_X = torch.tensor(\n",
    "    range(num_alternatives), dtype=torch.float\n",
    ").repeat(num_train).view(-1, 1)\n",
    "train_Y = test_function(train_X)\n",
    "\n",
    "all_alternatives = train_X[:num_alternatives].clone()\n",
    "\n",
    "num_models = 5\n",
    "emb_dim = 2\n",
    "pre_train_embs = torch.zeros(num_models, num_alternatives, emb_dim)\n",
    "post_train_embs = torch.zeros(num_models, num_alternatives, emb_dim)\n",
    "post_train_mean = torch.zeros(num_models, num_alternatives)\n",
    "post_train_covar = torch.zeros(num_models, num_alternatives, num_alternatives)\n",
    "mll_vals = torch.zeros(num_models)\n",
    "for i in range(num_models):\n",
    "    model = LCEGP(\n",
    "        train_X,\n",
    "        train_Y,\n",
    "        categorical_cols=[0],\n",
    "        embs_dim_list=[emb_dim],\n",
    "    )\n",
    "    pre_train_embs[i] = model.emb_layers[0].weight.detach().clone()\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    fit_gpytorch_model(mll)\n",
    "    post_train_embs[i] = model.emb_layers[0].weight.detach()\n",
    "    post_train_covar[i] = model.posterior(all_alternatives).mvn.covariance_matrix.detach()\n",
    "    post_train_mean[i] = model.posterior(all_alternatives).mvn.mean.detach()\n",
    "    mll_vals[i] = mll(model(train_X), train_Y.squeeze())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.3565,  1.5856],\n         [-0.9723,  0.9848],\n         [-0.3298, -1.0075],\n         [ 0.1390, -0.3247],\n         [-0.3806,  0.2045]],\n\n        [[-0.2174, -1.1883],\n         [-2.0870, -0.7230],\n         [ 0.7595,  1.5391],\n         [ 0.5531, -0.5249],\n         [ 0.2064, -1.0177]],\n\n        [[-0.8223,  0.1493],\n         [-0.1904, -0.5873],\n         [ 0.8683,  0.3399],\n         [ 0.9867, -0.5936],\n         [ 0.3958,  1.2221]],\n\n        [[-0.7546,  0.3082],\n         [-0.0671, -1.1361],\n         [-0.0031,  0.5324],\n         [-1.5114,  0.1558],\n         [ 1.1944, -1.4808]],\n\n        [[ 0.7357, -0.9264],\n         [-0.1551, -0.4375],\n         [-0.3579,  2.2555],\n         [ 0.6632,  1.8686],\n         [ 0.7138, -0.3822]]])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_embs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.0660,  0.1782],\n         [-0.9904,  0.4270],\n         [-1.0597,  0.5120],\n         [-0.2218,  0.0702],\n         [ 0.4376,  0.2553]],\n\n        [[-0.0799, -0.2900],\n         [-0.8112,  0.5199],\n         [-1.1827,  0.7715],\n         [-0.1585, -0.2056],\n         [ 1.4468, -2.7107]],\n\n        [[-0.1267,  0.6344],\n         [ 0.2378,  0.1385],\n         [ 0.2541,  0.1161],\n         [ 1.3652, -1.6436],\n         [-0.4923,  1.2851]],\n\n        [[-1.1714,  0.2884],\n         [-0.5324, -0.2406],\n         [-0.5187, -0.2507],\n         [-0.9312,  0.2107],\n         [ 2.0118, -1.6284]],\n\n        [[ 0.4180,  0.3470],\n         [-0.3673,  0.8043],\n         [-0.3925,  0.8191],\n         [ 0.3346,  0.3955],\n         [ 1.6068,  0.0122]]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_train_embs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 3.6470e-03, -5.1081e-05,  3.1012e-04,  3.0254e-03,  1.4051e-03],\n         [-5.1081e-05,  4.0724e-03,  3.8931e-03,  4.7308e-04, -2.3741e-04],\n         [ 3.1012e-04,  3.8931e-03,  4.1052e-03,  2.3544e-05, -1.4371e-04],\n         [ 3.0254e-03,  4.7308e-04,  2.3663e-05,  4.9225e-03, -2.0510e-04],\n         [ 1.4051e-03, -2.3735e-04, -1.4371e-04, -2.0504e-04,  7.2671e-03]],\n\n        [[ 3.5106e-03,  1.8239e-04, -2.5272e-05,  3.5221e-03,  7.6634e-04],\n         [ 1.8245e-04,  3.6743e-03,  3.8066e-03,  4.3255e-04, -2.5892e-04],\n         [-2.5272e-05,  3.8068e-03,  3.9825e-03,  1.9294e-04, -1.3566e-04],\n         [ 3.5221e-03,  4.3255e-04,  1.9288e-04,  3.6368e-03,  1.8078e-04],\n         [ 7.6646e-04, -2.5904e-04, -1.3572e-04,  1.8066e-04,  7.2054e-03]],\n\n        [[ 6.6063e-03,  5.9146e-04,  9.4950e-05, -1.5658e-04,  9.4891e-04],\n         [ 5.9140e-04,  3.8016e-03,  3.8866e-03,  1.9610e-05, -2.7776e-04],\n         [ 9.4950e-05,  3.8866e-03,  4.0587e-03,  1.0818e-04, -1.1563e-04],\n         [-1.5658e-04,  1.9610e-05,  1.0824e-04,  7.9356e-03,  8.2560e-05],\n         [ 9.4885e-04, -2.7770e-04, -1.1563e-04,  8.2560e-05,  7.3180e-03]],\n\n        [[ 4.7626e-03, -2.1112e-04, -5.1856e-05,  3.4989e-03,  5.4312e-06],\n         [-2.1112e-04,  3.9951e-03,  3.9574e-03,  3.8952e-04, -4.5244e-06],\n         [-5.1856e-05,  3.9574e-03,  4.1798e-03, -2.7180e-05,  8.3186e-06],\n         [ 3.4989e-03,  3.8952e-04, -2.7180e-05,  4.3420e-03, -7.7221e-06],\n         [ 5.4315e-06, -4.5225e-06,  8.3167e-06, -7.7211e-06,  8.0529e-03]],\n\n        [[ 3.4893e-03,  1.8674e-04, -8.8811e-06,  3.4996e-03,  7.8952e-04],\n         [ 1.8680e-04,  3.6737e-03,  3.8012e-03,  4.3672e-04, -2.6524e-04],\n         [-8.9407e-06,  3.8012e-03,  3.9651e-03,  2.1297e-04, -1.5169e-04],\n         [ 3.4996e-03,  4.3672e-04,  2.1309e-04,  3.6088e-03,  2.0683e-04],\n         [ 7.8946e-04, -2.6524e-04, -1.5175e-04,  2.0689e-04,  7.1736e-03]]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_train_covar"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.0337,  0.9436,  0.9352,  0.2040, -0.5264],\n        [ 0.0760,  0.9292,  0.9505,  0.1760, -0.5417],\n        [ 0.0741,  0.9307,  0.9472,  0.1814, -0.5421],\n        [ 0.0910,  0.9344,  0.9485,  0.1685, -0.5505],\n        [ 0.0756,  0.9298,  0.9503,  0.1756, -0.5405]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_train_mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-0.2153, -0.2085, -0.2156, -0.2120, -0.2085], grad_fn=<CopySlices>)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mll_vals"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We indeed observe some significant differences in model fits even when we use the same\n",
    "training data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}