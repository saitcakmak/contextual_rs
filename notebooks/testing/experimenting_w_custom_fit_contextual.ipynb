{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the classical RS setting of only categorical variables, there doesn't seem to be any\n",
    "advantage to using custom fit. This notebook considers the contextual case, where some\n",
    "other experiments suggest that custom fit may be helping somewhat significantly.\n",
    "\n",
    "Below, we use a version of custom_fit that returns additional output for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "from gpytorch import ExactMarginalLogLikelihood\n",
    "from torch import Tensor\n",
    "from contextual_rs.lce_gp import LCEGP\n",
    "\n",
    "\n",
    "import math\n",
    "from collections import Callable\n",
    "from copy import deepcopy\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "from botorch.fit import _set_transformed_inputs\n",
    "from botorch.optim.fit import fit_gpytorch_scipy\n",
    "from botorch.optim.utils import sample_all_priors\n",
    "from contextual_rs.models.lce_gp import LCEGP\n",
    "from gpytorch.mlls import MarginalLogLikelihood\n",
    "\n",
    "\n",
    "def custom_fit_gpytorch_model(\n",
    "    mll: MarginalLogLikelihood, optimizer: Callable = fit_gpytorch_scipy, **kwargs: Any\n",
    ") -> Tuple[MarginalLogLikelihood, int, float]:\n",
    "    r\"\"\"\n",
    "    This is a modified version of BoTorch `fit_gpytorch_model`. `fit_gpytorch_model`\n",
    "    has some inconsistent behavior in fitting the embedding weights in LCEGP.\n",
    "    The idea here is to get around this issue by aiming for a global fit.\n",
    "\n",
    "    Args:\n",
    "        mll: The marginal log-likelihood of the model. To be maximized.\n",
    "        optimizer: The optimizer for optimizing the mll starting from an\n",
    "            initialization of model parameters.\n",
    "        **kwargs: Optional arguments.\n",
    "\n",
    "    Returns:\n",
    "        The optimized mll.\n",
    "    \"\"\"\n",
    "    assert isinstance(mll.model, LCEGP), \"Only supports LCEGP!\"\n",
    "    num_retries = kwargs.pop(\"num_retries\", 1)\n",
    "    mll.train()\n",
    "    original_state_dict = deepcopy(mll.model.state_dict())\n",
    "    retry = 0\n",
    "    state_dict_list = list()\n",
    "    mll_values = torch.zeros(num_retries)\n",
    "    max_error_tries = kwargs.pop(\"max_error_tries\", 10)\n",
    "    randn_factor = kwargs.pop(\"randn_factor\", 0.1)\n",
    "    error_count = 0\n",
    "    while retry < num_retries:\n",
    "        if retry > 0:  # use normal initial conditions on first try\n",
    "            mll.model.load_state_dict(original_state_dict)\n",
    "            # randomize the embedding as well, reinitializing here.\n",
    "            # two alternatives for initialization, specified by passing randn_factor\n",
    "            for i, emb_layer in enumerate(mll.model.emb_layers):\n",
    "                if randn_factor == 0:\n",
    "                    new_emb = torch.nn.Embedding(\n",
    "                        emb_layer.num_embeddings,\n",
    "                        emb_layer.embedding_dim,\n",
    "                        max_norm=emb_layer.max_norm,\n",
    "                    ).to(emb_layer.weight)\n",
    "                    mll.model.emb_layers[i] = new_emb\n",
    "                else:\n",
    "                    new_weight = torch.randn_like(emb_layer.weight) * randn_factor\n",
    "                    emb_layer.weight = torch.nn.Parameter(\n",
    "                        new_weight, requires_grad=True\n",
    "                    )\n",
    "            sample_all_priors(mll.model)\n",
    "        mll, info_dict = optimizer(mll, track_iterations=False, **kwargs)\n",
    "        opt_val = info_dict[\"fopt\"]\n",
    "        if math.isnan(opt_val):\n",
    "            if error_count < max_error_tries:\n",
    "                error_count += 1\n",
    "                continue\n",
    "            else:\n",
    "                state_dict_list.append(original_state_dict)\n",
    "                mll_values[retry] = float(\"-inf\")\n",
    "                retry += 1\n",
    "                continue\n",
    "\n",
    "        # record the fitted model and the corresponding mll value\n",
    "        state_dict_list.append(deepcopy(mll.model.state_dict()))\n",
    "        mll_values[retry] = -opt_val  # negate to get mll value\n",
    "        retry += 1\n",
    "\n",
    "    # pick the best among all trained models\n",
    "    best_idx = mll_values.argmax()\n",
    "    best_params = state_dict_list[best_idx]\n",
    "    mll.model.load_state_dict(best_params)\n",
    "    _set_transformed_inputs(mll=mll)\n",
    "    return mll.eval(), best_idx, mll_values[best_idx] - mll_values[0]\n",
    "\n",
    "\n",
    "def test_func(X: Tensor) -> Tensor:\n",
    "    assert X.dim() == 2\n",
    "    context_dim = X.shape[-1] - 1\n",
    "    part_1 = X[:, 0].view(-1, 1) * X[:, 1:].sum(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "\n",
    "def fit_on_random_model(\n",
    "    randn_factor: float,\n",
    "    fit_tries: int,\n",
    "    num_arms: int,\n",
    "    context_dim: int,\n",
    "    num_train: int,\n",
    "    seed: int,\n",
    ") -> Tuple[int, Tensor]:\n",
    "    torch.manual_seed(seed)\n",
    "    ckwargs = {\"dtype\": torch.double, \"device\": \"cpu\"}\n",
    "    train_X = torch.cat(\n",
    "        [\n",
    "            torch.randint(0, num_arms, (num_train, 1), **ckwargs),\n",
    "            torch.rand(num_train, context_dim, **ckwargs)\n",
    "        ], dim=-1\n",
    "    )\n",
    "    train_Y = torch.randn()\n",
    "\n",
    "    model = LCEGP(train_X, train_Y, categorical_cols=[0])\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    fitted_mll, best_idx, improvement = custom_fit_gpytorch_model(\n",
    "        mll, num_retries=fit_tries, randn_factor=randn_factor\n",
    "    )\n",
    "    return best_idx, improvement\n",
    "\n",
    "\n",
    "def multi_fit(\n",
    "    replications: int,\n",
    "    **kwargs,\n",
    ") -> Tuple[float, Tensor]:\n",
    "    r\"\"\"\n",
    "    Runs fit_on_random_model multiple times and returns the fraction of time the\n",
    "    best_idx is non-zero and the average improvement over simple fit.\n",
    "    \"\"\"\n",
    "    best_idcs = torch.zeros(replications)\n",
    "    improvements = torch.zeros(replications)\n",
    "    for seed in range(replications):\n",
    "        best_idcs[seed], improvements[seed] = fit_on_random_model(**kwargs, seed=seed)\n",
    "    return best_idcs.bool().to(torch.float).mean(), improvements.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that the setup is ready, let's run some alternatives under several settings.\n",
    "\n",
    "Alternatives to consider: 0 (default initialization), 1.0, 0.5, 0.1, 0.05"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative 0, fraction 0.25, improvement 0.004703551530838013\n",
      "Alternative 1.0, fraction 0.25, improvement 0.004703551530838013\n",
      "Alternative 0.5, fraction 0.25, improvement 0.004703551530838013\n",
      "Alternative 0.1, fraction 0.25, improvement 0.004703551530838013\n",
      "Alternative 0.05, fraction 0.25, improvement 0.004703551530838013\n"
     ]
    }
   ],
   "source": [
    "alternatives = [0, 1.0, 0.5, 0.1, 0.05]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"fit_tries\": 10,\n",
    "    \"num_alternatives\": 5,\n",
    "    \"num_train\": 10,\n",
    "    \"replications\": 20,\n",
    "}\n",
    "\n",
    "for randn_factor in alternatives:\n",
    "    fraction, improvement = multi_fit(**kwargs, randn_factor=randn_factor)\n",
    "    print(f\"Alternative {randn_factor}, fraction {fraction}, improvement {improvement}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative 0, fraction 0.699999988079071, improvement 0.007884478196501732\n",
      "Alternative 1.0, fraction 0.699999988079071, improvement 0.007884478196501732\n",
      "Alternative 0.5, fraction 0.699999988079071, improvement 0.007884478196501732\n",
      "Alternative 0.1, fraction 0.699999988079071, improvement 0.007884478196501732\n",
      "Alternative 0.05, fraction 0.699999988079071, improvement 0.007884478196501732\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\n",
    "    \"fit_tries\": 10,\n",
    "    \"num_alternatives\": 10,\n",
    "    \"num_train\": 10,\n",
    "    \"replications\": 20,\n",
    "}\n",
    "\n",
    "for randn_factor in alternatives:\n",
    "    fraction, improvement = multi_fit(**kwargs, randn_factor=randn_factor)\n",
    "    print(f\"Alternative {randn_factor}, fraction {fraction}, improvement {improvement}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative 0, fraction 0.8500000238418579, improvement 0.009538346901535988\n",
      "Alternative 1.0, fraction 0.8999999761581421, improvement 0.012168830260634422\n",
      "Alternative 0.5, fraction 0.949999988079071, improvement 0.012226665392518044\n",
      "Alternative 0.1, fraction 0.949999988079071, improvement 0.012226665392518044\n",
      "Alternative 0.05, fraction 0.949999988079071, improvement 0.012226665392518044\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\n",
    "    \"fit_tries\": 10,\n",
    "    \"num_alternatives\": 20,\n",
    "    \"num_train\": 5,\n",
    "    \"replications\": 20,\n",
    "}\n",
    "\n",
    "for randn_factor in alternatives:\n",
    "    fraction, improvement = multi_fit(**kwargs, randn_factor=randn_factor)\n",
    "    print(f\"Alternative {randn_factor}, fraction {fraction}, improvement {improvement}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative 0, fraction 0.6499999761581421, improvement 0.005162292625755072\n",
      "Alternative 1.0, fraction 0.949999988079071, improvement 0.010405289940536022\n",
      "Alternative 0.5, fraction 1.0, improvement 0.011099308729171753\n",
      "Alternative 0.1, fraction 1.0, improvement 0.011149173602461815\n",
      "Alternative 0.05, fraction 1.0, improvement 0.011149173602461815\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\n",
    "    \"fit_tries\": 10,\n",
    "    \"num_alternatives\": 40,\n",
    "    \"num_train\": 5,\n",
    "    \"replications\": 20,\n",
    "}\n",
    "\n",
    "for randn_factor in alternatives:\n",
    "    fraction, improvement = multi_fit(**kwargs, randn_factor=randn_factor)\n",
    "    print(f\"Alternative {randn_factor}, fraction {fraction}, improvement {improvement}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative 0, fraction 0.6499999761581421, improvement 0.0036636709701269865\n",
      "Alternative 1.0, fraction 0.6499999761581421, improvement 0.0036636709701269865\n",
      "Alternative 0.5, fraction 0.6499999761581421, improvement 0.0036636709701269865\n",
      "Alternative 0.1, fraction 0.6499999761581421, improvement 0.0036636709701269865\n",
      "Alternative 0.05, fraction 0.6499999761581421, improvement 0.0036636709701269865\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\n",
    "    \"fit_tries\": 10,\n",
    "    \"num_alternatives\": 10,\n",
    "    \"num_train\": 30,\n",
    "    \"replications\": 20,\n",
    "}\n",
    "\n",
    "for randn_factor in alternatives:\n",
    "    fraction, improvement = multi_fit(**kwargs, randn_factor=randn_factor)\n",
    "    print(f\"Alternative {randn_factor}, fraction {fraction}, improvement {improvement}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}