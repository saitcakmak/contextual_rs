{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This notebook demonstrates the randomness in the embedding fit when using\n",
    "`fit_gpytorch_model` to train the model. The randomness is due to random\n",
    "initialization of the embedding weights.\n",
    "\n",
    "This variance in the embedding results in significant variance in the posterior\n",
    "mean and covariance.\n",
    "\n",
    "Below, we fit 5 instances of LCEMGP on the same training data to demonstrate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from botorch import fit_gpytorch_model\n",
    "from botorch.models.contextual_multioutput import LCEMGP\n",
    "from gpytorch import ExactMarginalLogLikelihood\n",
    "from torch import Tensor\n",
    "\n",
    "def test_function(X: Tensor) -> Tensor:\n",
    "    sine = torch.sin(X)\n",
    "    linear = X * 0.05\n",
    "    noise = torch.randn_like(X) * 0.25\n",
    "    return (sine + linear + noise).sum(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "num_alternatives = 5\n",
    "num_train = 5\n",
    "train_X_cat = torch.tensor(\n",
    "    range(num_alternatives), dtype=torch.float\n",
    ").repeat(num_train).view(-1, 1)\n",
    "train_X = torch.cat(\n",
    "    [train_X_cat, torch.rand_like(train_X_cat)], dim=-1\n",
    ")\n",
    "train_Y = test_function(train_X)\n",
    "\n",
    "num_models = 5\n",
    "emb_dim = 2\n",
    "pre_train_embs = torch.zeros(num_models, num_alternatives, emb_dim)\n",
    "post_train_embs = torch.zeros(num_models, num_alternatives, emb_dim)\n",
    "post_train_mean = torch.zeros(num_models, num_alternatives)\n",
    "post_train_covar = torch.zeros(num_models, num_alternatives, num_alternatives)\n",
    "for i in range(num_models):\n",
    "    model = LCEMGP(\n",
    "        train_X,\n",
    "        train_Y,\n",
    "        task_feature=0,\n",
    "        embs_dim_list=[emb_dim],\n",
    "    )\n",
    "    pre_train_embs[i] = model.emb_layers[0].weight.detach()\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    fit_gpytorch_model(mll)\n",
    "    post_train_embs[i] = model.emb_layers[0].weight.detach()\n",
    "    posterior_mvn = model.posterior(torch.ones(1, 1) * 0.5).mvn\n",
    "    post_train_covar[i] = posterior_mvn.covariance_matrix.detach()\n",
    "    post_train_mean[i] = posterior_mvn.mean.detach()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The random initialization of the embedding (pre-train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-1.9477, -0.9121],\n         [-0.8366, -1.3235],\n         [ 1.3058,  1.9682],\n         [ 1.1083,  0.3799],\n         [ 0.0707,  1.8554]],\n\n        [[-1.0116, -0.7351],\n         [ 0.8207, -0.4530],\n         [-1.0873,  0.3601],\n         [ 0.3244,  1.5122],\n         [-1.0946, -1.1705]],\n\n        [[ 0.0567,  0.1575],\n         [-0.3365,  0.0523],\n         [ 0.3112, -0.5188],\n         [ 0.7687,  1.1599],\n         [-0.2045,  0.8756]],\n\n        [[ 1.6935,  0.0215],\n         [ 0.9371,  0.1164],\n         [-1.4184, -1.5826],\n         [ 1.0568, -0.1078],\n         [-0.2043, -2.5470]],\n\n        [[ 1.5344,  0.5198],\n         [-1.5981,  2.2358],\n         [-0.4685, -0.0651],\n         [-1.1122, -0.9644],\n         [ 0.5034,  1.9493]]])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_embs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The fitted embedding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.5859, -0.1187],\n         [-0.6807, -1.7474],\n         [ 1.6677,  1.5944],\n         [ 1.9523,  0.0944],\n         [-2.6530,  2.1454]],\n\n        [[-0.1066,  0.1133],\n         [ 0.8881,  1.0305],\n         [ 0.9482,  1.0858],\n         [ 0.1192,  0.3120],\n         [-3.8973, -3.0280]],\n\n        [[ 0.1004,  0.2506],\n         [ 0.4040, -0.4959],\n         [ 0.4061, -0.5003],\n         [ 0.1939,  0.0218],\n         [-0.5089,  2.4504]],\n\n        [[ 0.9000, -0.4527],\n         [ 0.8141,  0.2153],\n         [-1.8824, -0.9803],\n         [ 0.8719, -0.2278],\n         [ 1.3612, -2.6539]],\n\n        [[ 1.3934,  0.5379],\n         [-1.5487,  0.4246],\n         [-1.1437,  0.3254],\n         [-0.4918,  0.5832],\n         [ 0.6499,  1.8043]]])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_train_embs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Posterior covariance (evaluated at X=0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 6.3067e-02,  1.3132e-02,  1.0758e-03, -1.5726e-04,  2.0660e-02],\n         [ 1.3132e-02,  3.6272e-02,  5.6101e-04,  1.9624e-04,  9.3919e-04],\n         [ 1.0758e-03,  5.6101e-04,  6.3291e-02,  1.9678e-02,  2.2500e-04],\n         [-1.5726e-04,  1.9624e-04,  1.9678e-02,  5.8053e-02, -3.3465e-06],\n         [ 2.0660e-02,  9.3922e-04,  2.2499e-04, -3.3460e-06,  4.7394e-02]],\n\n        [[ 6.1665e-02,  4.4771e-03,  4.5211e-03,  4.5334e-02,  1.4315e-02],\n         [ 4.4771e-03,  2.4659e-02,  2.4630e-02,  9.7251e-03, -7.5728e-05],\n         [ 4.5210e-03,  2.4631e-02,  2.4651e-02,  9.7865e-03, -7.2449e-05],\n         [ 4.5335e-02,  9.7250e-03,  9.7866e-03,  4.5851e-02,  5.1273e-03],\n         [ 1.4315e-02, -7.5802e-05, -7.2479e-05,  5.1274e-03,  5.2308e-02]],\n\n        [[ 6.0611e-02,  4.4552e-03,  4.2512e-03,  4.5031e-02,  1.5132e-02],\n         [ 4.4552e-03,  2.4687e-02,  2.4786e-02,  9.8573e-03, -6.7830e-05],\n         [ 4.2512e-03,  2.4786e-02,  2.4898e-02,  9.4204e-03, -4.2126e-05],\n         [ 4.5031e-02,  9.8575e-03,  9.4204e-03,  4.5862e-02,  5.4199e-03],\n         [ 1.5133e-02, -6.7815e-05, -4.2126e-05,  5.4199e-03,  5.2087e-02]],\n\n        [[ 4.7542e-02,  4.0969e-03,  3.5974e-04,  3.5345e-02,  1.4980e-02],\n         [ 4.0969e-03,  3.1038e-02,  1.1508e-03,  1.1969e-02,  7.9161e-04],\n         [ 3.5974e-04,  1.1509e-03,  7.1156e-02,  1.3919e-03,  5.1802e-03],\n         [ 3.5345e-02,  1.1969e-02,  1.3919e-03,  3.3847e-02,  8.3735e-03],\n         [ 1.4980e-02,  7.9161e-04,  5.1802e-03,  8.3734e-03,  4.4317e-02]],\n\n        [[ 9.1738e-02,  5.8231e-04,  5.5274e-04,  7.2888e-04,  1.9666e-02],\n         [ 5.8232e-04,  2.4857e-02,  2.4731e-02,  1.0344e-02,  9.2179e-05],\n         [ 5.5275e-04,  2.4731e-02,  2.4635e-02,  1.0759e-02,  1.1697e-04],\n         [ 7.2888e-04,  1.0344e-02,  1.0759e-02,  4.2991e-02,  1.1433e-02],\n         [ 1.9666e-02,  9.2089e-05,  1.1697e-04,  1.1433e-02,  4.4303e-02]]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_train_covar"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Posterior mean (evaluated at X=0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.5603,  1.4518,  1.4640,  0.7985, -0.1114],\n        [ 0.3183,  1.4759,  1.4752,  0.6964, -0.1422],\n        [ 0.3207,  1.4744,  1.4787,  0.7013, -0.1402],\n        [ 0.3730,  1.4577,  1.6185,  0.7435, -0.1573],\n        [ 0.4247,  1.4884,  1.4833,  0.7414, -0.1271]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_train_mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}