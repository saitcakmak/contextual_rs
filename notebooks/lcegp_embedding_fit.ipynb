{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this notebook, we take a closer look into how the embedding of LCEGP changes as we\n",
    "add more samples and re-train the model.\n",
    "\n",
    "In the experiments, we observe some wild swings in the LCEGP performance between each\n",
    "model fit. We want to understand why that happens, and figure out if we can get rid of\n",
    "the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "from botorch import fit_gpytorch_model\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.models.transforms import Standardize\n",
    "from botorch.test_functions import Griewank\n",
    "from botorch.utils.transforms import unnormalize\n",
    "from gpytorch import ExactMarginalLogLikelihood\n",
    "from torch import Tensor\n",
    "\n",
    "from contextual_rs.models.lce_gp import LCEGP\n",
    "from contextual_rs.models.custom_fit import custom_fit_gpytorch_model\n",
    "from contextual_rs.models.contextual_independent_model import ContextualIndependentModel\n",
    "\n",
    "\n",
    "function = Griewank(dim=2, negate=True, noise_std=5.0)\n",
    "function.bounds[0, :].fill_(-5)\n",
    "function.bounds[0, :].fill_(5)\n",
    "\n",
    "\n",
    "def eval_function(X: Tensor, evaluate_true: bool = False) -> Tensor:\n",
    "    if evaluate_true:\n",
    "        return function.evaluate_true(unnormalize(X, function.bounds))\n",
    "    else:\n",
    "        return function(unnormalize(X, function.bounds))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "num_arms = 4\n",
    "num_contexts = 6\n",
    "num_alternatives = num_arms * num_contexts\n",
    "num_full_train = 5\n",
    "arm_set = torch.arange(0, num_arms).view(-1, 1)\n",
    "context_set = torch.linspace(0, 1, num_contexts).view(-1, 1)\n",
    "\n",
    "lcegp_train_X = torch.cat(\n",
    "    [\n",
    "        arm_set.view(-1, 1, 1).repeat(1, num_contexts, 1),\n",
    "        context_set.repeat(num_arms, 1, 1),\n",
    "    ], dim=-1\n",
    ").view(-1, 2).repeat(num_full_train, 1)\n",
    "st_train_X = lcegp_train_X.clone()\n",
    "st_train_X[..., 0] = st_train_X[..., 0] / (num_arms - 1)\n",
    "indep_train_X = lcegp_train_X.clone()\n",
    "indep_train_X[..., 1] = indep_train_X[..., 1] * (num_contexts - 1)\n",
    "train_Y = eval_function(st_train_X).view(-1, 1)\n",
    "\n",
    "true_mean = eval_function(\n",
    "    st_train_X[:num_alternatives], evaluate_true=True\n",
    ").view(num_arms, num_contexts)\n",
    "\n",
    "lcegp = LCEGP(\n",
    "    lcegp_train_X, train_Y, [0], outcome_transform=Standardize(m=1)\n",
    ")\n",
    "mll = ExactMarginalLogLikelihood(lcegp.likelihood, lcegp)\n",
    "custom_fit_gpytorch_model(mll)\n",
    "\n",
    "stgp = SingleTaskGP(\n",
    "    st_train_X, train_Y, outcome_transform=Standardize(m=1)\n",
    ")\n",
    "mll = ExactMarginalLogLikelihood(stgp.likelihood, stgp)\n",
    "fit_gpytorch_model(mll)\n",
    "\n",
    "indep_model = ContextualIndependentModel(\n",
    "    indep_train_X, train_Y.squeeze(-1)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's take a look at the posterior means as predicted by these models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arm idx: 0\n",
      "True mean: tensor([ 1.0125,  4.8503, 15.7685, 33.7673, 58.8465, 91.0062])\n",
      "Indep mean: tensor([ -1.9953,  -7.2004, -17.4813, -30.2895, -60.3034, -91.6836])\n",
      "LCEGP mean: tensor([ -2.1314,  -6.4410, -16.5430, -32.9897, -59.8297, -91.6104],\n",
      "       grad_fn=<SelectBackward>)\n",
      "ST mean: tensor([ -2.1464,  -7.2155, -16.3478, -32.2198, -59.4672, -91.6482],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Arm idx: 1\n",
      "True mean: tensor([ 11.3424,  15.1801,  26.0984,  44.0971,  69.1764, 101.3361])\n",
      "Indep mean: tensor([  -9.0395,  -16.4518,  -24.2740,  -45.4381,  -69.1548, -101.6101])\n",
      "LCEGP mean: tensor([ -10.8063,  -15.0129,  -25.7578,  -43.0279,  -70.0538, -101.2419],\n",
      "       grad_fn=<SelectBackward>)\n",
      "ST mean: tensor([  -9.9959,  -14.9000,  -25.7136,  -44.2292,  -70.0255, -100.9696],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Arm idx: 2\n",
      "True mean: tensor([ 41.3403,  45.1780,  56.0963,  74.0950,  99.1743, 131.3340])\n",
      "Indep mean: tensor([ -45.0957,  -44.0481,  -58.8242,  -75.4303, -101.1235, -128.0902])\n",
      "LCEGP mean: tensor([ -43.2777,  -46.4584,  -57.7531,  -75.6761, -100.9594, -128.0442],\n",
      "       grad_fn=<SelectBackward>)\n",
      "ST mean: tensor([ -44.1353,  -45.8967,  -57.6356,  -75.8502, -100.3139, -129.4297],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Arm idx: 3\n",
      "True mean: tensor([ 91.0062,  94.8440, 105.7623, 123.7610, 148.8402, 181.0000])\n",
      "Indep mean: tensor([ -90.5894,  -92.1203, -106.7449, -118.7417, -147.5214, -182.4544])\n",
      "LCEGP mean: tensor([ -90.1758,  -93.4959, -104.7816, -120.1177, -147.8159, -181.6598],\n",
      "       grad_fn=<SelectBackward>)\n",
      "ST mean: tensor([ -90.2049,  -93.4239, -104.3190, -120.7246, -147.8792, -181.0318],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "lcegp_pm = lcegp.posterior(lcegp_train_X[:num_alternatives]).mean.view(num_arms, num_contexts)\n",
    "st_pm = stgp.posterior(st_train_X[:num_alternatives]).mean.view(num_arms, num_contexts)\n",
    "indep_pm = indep_model.means.view(num_arms, num_contexts)\n",
    "\n",
    "for arm_idx in range(num_arms):\n",
    "    print(f\"Arm idx: {arm_idx}\")\n",
    "    print(f\"True mean: {true_mean[arm_idx]}\")\n",
    "    print(f\"Indep mean: {indep_pm[arm_idx]}\")\n",
    "    print(f\"LCEGP mean: {lcegp_pm[arm_idx]}\")\n",
    "    print(f\"ST mean: {st_pm[arm_idx]}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LCEGP seems to deviate from the empirical mean slightly more than others. Let's fit\n",
    "each model on the same data multiple times and see how much the posterior mean varies\n",
    "between attempts.\n",
    "\n",
    "We will not re-fit the independent model since it is deterministic (sample mean)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def fit_replications(replications: int, fit_tries: int = 1) -> Tuple[Tensor, Tensor]:\n",
    "    lcegp_pms = torch.zeros(replications, num_arms, num_contexts)\n",
    "    st_pms = torch.zeros(replications, num_arms, num_contexts)\n",
    "    for rep in range(replications):\n",
    "        lcegp = LCEGP(\n",
    "            lcegp_train_X, train_Y, [0], outcome_transform=Standardize(m=1)\n",
    "        )\n",
    "        mll = ExactMarginalLogLikelihood(lcegp.likelihood, lcegp)\n",
    "        custom_fit_gpytorch_model(mll, num_retries=fit_tries)\n",
    "\n",
    "        stgp = SingleTaskGP(\n",
    "            st_train_X, train_Y, outcome_transform=Standardize(m=1)\n",
    "        )\n",
    "        mll = ExactMarginalLogLikelihood(stgp.likelihood, stgp)\n",
    "        fit_gpytorch_model(mll)\n",
    "\n",
    "        lcegp_pms[rep] = lcegp.posterior(\n",
    "            lcegp_train_X[:num_alternatives]\n",
    "        ).mean.view(num_arms, num_contexts)\n",
    "        st_pms[rep] = stgp.posterior(\n",
    "            st_train_X[:num_alternatives]\n",
    "        ).mean.view(num_arms, num_contexts)\n",
    "    return lcegp_pms, st_pms"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcegp avg std: 0.5305986404418945\n",
      "st avg std: 0.0\n",
      "lcegp avg mse: 2.3495187759399414\n",
      "st avg mse: 1.4980558156967163\n",
      "lcegp avg normalized mse: 0.04930578172206879\n",
      "st avg normalized mse: 0.029212482273578644\n"
     ]
    }
   ],
   "source": [
    "lcegp_pms, st_pms = fit_replications(30)\n",
    "lcegp_stds = lcegp_pms.std(dim=0)\n",
    "st_stds = st_pms.std(dim=0)\n",
    "\n",
    "lcegp_diffs = lcegp_pms - indep_pm\n",
    "st_diffs = st_pms - indep_pm\n",
    "\n",
    "lcegp_mse = lcegp_diffs.pow(2).mean(dim=0)\n",
    "st_mse = st_diffs.pow(2).mean(dim=0)\n",
    "\n",
    "print(f\"lcegp avg std: {lcegp_stds.mean()}\")\n",
    "print(f\"st avg std: {st_stds.mean()}\")\n",
    "\n",
    "print(f\"lcegp avg mse: {lcegp_mse.mean()}\")\n",
    "print(f\"st avg mse: {st_mse.mean()}\")\n",
    "\n",
    "lcegp_mse_normalized = lcegp_mse.sqrt() / indep_pm.abs()\n",
    "st_mse_normalized = st_mse.sqrt() / indep_pm.abs()\n",
    "\n",
    "print(f\"lcegp avg normalized mse: {lcegp_mse_normalized.mean()}\")\n",
    "print(f\"st avg normalized mse: {st_mse_normalized.mean()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Depending on the data, LCEGP may have significantly larger MSE (re-run notebook if\n",
    "not obvious). It also clearly shows some deviation in posterior mean between repetitions,\n",
    "which STGP does not have. This suggests that we get \"local\" fit for the\n",
    "hyper-parameters, which depends on the initialization, rather than a global one.\n",
    "\n",
    "Before looking into the latent covariance directly, let's check how re-tries affect the\n",
    " behavior."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saitcakmak/anaconda3/envs/contextual_rs/lib/python3.8/site-packages/gpytorch/utils/cholesky.py:51: NumericalWarning: A not p.d., added jitter of 1.0e-04 to the diagonal\n",
      "  warnings.warn(f\"A not p.d., added jitter of {jitter_new:.1e} to the diagonal\", NumericalWarning)\n",
      "/home/saitcakmak/anaconda3/envs/contextual_rs/lib/python3.8/site-packages/gpytorch/utils/cholesky.py:51: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(f\"A not p.d., added jitter of {jitter_new:.1e} to the diagonal\", NumericalWarning)\n",
      "/home/saitcakmak/anaconda3/envs/contextual_rs/lib/python3.8/site-packages/gpytorch/utils/cholesky.py:51: NumericalWarning: A not p.d., added jitter of 1.0e-04 to the diagonal\n",
      "  warnings.warn(f\"A not p.d., added jitter of {jitter_new:.1e} to the diagonal\", NumericalWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcegp avg std: 0.30070069432258606\n",
      "st avg std: 0.0\n",
      "lcegp avg mse: 3.3249924182891846\n",
      "st avg mse: 1.4980558156967163\n",
      "lcegp avg normalized mse: 0.04955904558300972\n",
      "st avg normalized mse: 0.029212482273578644\n"
     ]
    }
   ],
   "source": [
    "lcegp_pms, st_pms = fit_replications(30, fit_tries=10)\n",
    "lcegp_stds = lcegp_pms.std(dim=0)\n",
    "st_stds = st_pms.std(dim=0)\n",
    "\n",
    "lcegp_diffs = lcegp_pms - indep_pm\n",
    "st_diffs = st_pms - indep_pm\n",
    "\n",
    "lcegp_mse = lcegp_diffs.pow(2).mean(dim=0)\n",
    "st_mse = st_diffs.pow(2).mean(dim=0)\n",
    "\n",
    "print(f\"lcegp avg std: {lcegp_stds.mean()}\")\n",
    "print(f\"st avg std: {st_stds.mean()}\")\n",
    "\n",
    "print(f\"lcegp avg mse: {lcegp_mse.mean()}\")\n",
    "print(f\"st avg mse: {st_mse.mean()}\")\n",
    "\n",
    "lcegp_mse_normalized = lcegp_mse.sqrt() / indep_pm.abs()\n",
    "st_mse_normalized = st_mse.sqrt() / indep_pm.abs()\n",
    "\n",
    "print(f\"lcegp avg normalized mse: {lcegp_mse_normalized.mean()}\")\n",
    "print(f\"st avg normalized mse: {st_mse_normalized.mean()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, re-tries reduce the \"std\", i.e., the variance between the posterior means across\n",
    "replications, but at least in this specific run, it lead to an increase in MSE. That's\n",
    "not that great.\n",
    "\n",
    "Could inferred noise level be playing a role here? Maybe LCEGP attributes too much to\n",
    "noise."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def get_fitted_noise(replications: int) -> Tuple[Tensor, Tensor]:\n",
    "    lcegp_noise = torch.zeros(replications)\n",
    "    st_noise = torch.zeros(replications)\n",
    "    for rep in range(replications):\n",
    "        lcegp = LCEGP(\n",
    "            lcegp_train_X, train_Y, [0], outcome_transform=Standardize(m=1)\n",
    "        )\n",
    "        mll = ExactMarginalLogLikelihood(lcegp.likelihood, lcegp)\n",
    "        custom_fit_gpytorch_model(mll)\n",
    "\n",
    "        stgp = SingleTaskGP(\n",
    "            st_train_X, train_Y, outcome_transform=Standardize(m=1)\n",
    "        )\n",
    "        mll = ExactMarginalLogLikelihood(stgp.likelihood, stgp)\n",
    "        fit_gpytorch_model(mll)\n",
    "\n",
    "        lcegp_noise[rep] = lcegp.likelihood.noise\n",
    "        st_noise[rep] = stgp.likelihood.noise\n",
    "    return lcegp_noise, st_noise"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCEGP noise, mean: 0.011341067031025887, std: 0.0010141489328816533\n",
      "STGP noise, mean: 0.011098301038146019, std: 0.0\n"
     ]
    }
   ],
   "source": [
    "lcegp_noise, st_noise = get_fitted_noise(30)\n",
    "print(f\"LCEGP noise, mean: {lcegp_noise.mean()}, std: {lcegp_noise.std()}\")\n",
    "print(f\"STGP noise, mean: {st_noise.mean()}, std: {st_noise.std()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Noise doesn't seem to be the reason here. Let's look into the arm covariance matrix.\n",
    "\n",
    "Note that what we calculate below is the prior covariance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from botorch.optim.fit import fit_gpytorch_torch\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "\n",
    "def get_arm_covariance(\n",
    "    replications: int, fit_tries: int = 1, adam: bool = False, init_method: str = None,\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    arm_covars = list()\n",
    "    for rep in range(replications):\n",
    "        torch.manual_seed(rep)\n",
    "        lcegp = LCEGP(\n",
    "            lcegp_train_X, train_Y, [0], outcome_transform=Standardize(m=1)\n",
    "        )\n",
    "        emb_layer = lcegp.emb_layers[0]\n",
    "        if init_method == \"rand\":\n",
    "            new_weight = torch.rand_like(emb_layer.weight)\n",
    "            emb_layer.weight = torch.nn.Parameter(\n",
    "                new_weight, requires_grad=True\n",
    "            )\n",
    "        elif init_method == \"gp\":\n",
    "            covar = lcegp.emb_covar_module\n",
    "            latent_covar = covar(\n",
    "                torch.linspace(\n",
    "                    0.0,\n",
    "                    1.0,\n",
    "                    num_arms,\n",
    "                )\n",
    "            ).add_jitter(1e-4)\n",
    "            latent_dist = MultivariateNormal(\n",
    "                torch.zeros(num_arms),\n",
    "                latent_covar,\n",
    "            )\n",
    "            latent_sample = latent_dist.sample().reshape(emb_layer.weight.shape)\n",
    "            emb_layer.weight = torch.nn.Parameter(\n",
    "                latent_sample, requires_grad=True\n",
    "            )\n",
    "            # They also register the distribution as a prior but I am not sure how to\n",
    "            # do that there.\n",
    "        mll = ExactMarginalLogLikelihood(lcegp.likelihood, lcegp)\n",
    "        if adam:\n",
    "            custom_fit_gpytorch_model(\n",
    "                mll,\n",
    "                optimizer=fit_gpytorch_torch,\n",
    "                num_retries=fit_tries,\n",
    "                options={\"disp\": False},\n",
    "            )\n",
    "        else:\n",
    "            custom_fit_gpytorch_model(mll, num_retries=fit_tries)\n",
    "\n",
    "        arms = arm_set.long().view(-1)\n",
    "        x_emb = lcegp.emb_layers[0](arms)\n",
    "        covar_emb = lcegp.emb_covar_module(x_emb)\n",
    "        arm_covars.append(covar_emb.evaluate())\n",
    "    return torch.stack(arm_covars)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean covariance: tensor([[1.0000, 0.7754, 0.7982, 0.5545],\n",
      "        [0.7754, 1.0000, 0.7179, 0.5509],\n",
      "        [0.7982, 0.7179, 1.0000, 0.6659],\n",
      "        [0.5545, 0.5509, 0.6659, 1.0000]], grad_fn=<MeanBackward1>)\n",
      "covariance std: tensor([[0.0000, 0.3944, 0.2867, 0.2996],\n",
      "        [0.3944, 0.0000, 0.3289, 0.3299],\n",
      "        [0.2867, 0.3289, 0.0000, 0.3613],\n",
      "        [0.2996, 0.3299, 0.3613, 0.0000]], grad_fn=<StdBackward1>)\n"
     ]
    }
   ],
   "source": [
    "arm_covariances = get_arm_covariance(30)\n",
    "\n",
    "print(f\"mean covariance: {arm_covariances.mean(dim=0)}\")\n",
    "print(f\"covariance std: {arm_covariances.std(dim=0)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With Adam"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean covariance: tensor([[1.0000, 0.7944, 0.8041, 0.5593],\n",
      "        [0.7944, 1.0000, 0.7910, 0.5557],\n",
      "        [0.8041, 0.7910, 1.0000, 0.7569],\n",
      "        [0.5593, 0.5557, 0.7569, 1.0000]], grad_fn=<MeanBackward1>)\n",
      "covariance std: tensor([[0.0000, 0.3967, 0.2709, 0.2243],\n",
      "        [0.3967, 0.0000, 0.3376, 0.2775],\n",
      "        [0.2709, 0.3376, 0.0000, 0.3102],\n",
      "        [0.2243, 0.2775, 0.3102, 0.0000]], grad_fn=<StdBackward1>)\n"
     ]
    }
   ],
   "source": [
    "arm_covariances = get_arm_covariance(30, 1, True)\n",
    "\n",
    "print(f\"mean covariance: {arm_covariances.mean(dim=0)}\")\n",
    "print(f\"covariance std: {arm_covariances.std(dim=0)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Indeed, the fitted covariance is quite variable between different tries.\n",
    "\n",
    "Let's try the same thing with multiple fit tries."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saitcakmak/anaconda3/envs/contextual_rs/lib/python3.8/site-packages/gpytorch/utils/cholesky.py:51: NumericalWarning: A not p.d., added jitter of 1.0e-04 to the diagonal\n",
      "  warnings.warn(f\"A not p.d., added jitter of {jitter_new:.1e} to the diagonal\", NumericalWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean covariance: tensor([[1.0000, 0.9974, 0.9588, 0.8130],\n",
      "        [0.9974, 1.0000, 0.9720, 0.8429],\n",
      "        [0.9588, 0.9720, 1.0000, 0.9065],\n",
      "        [0.8130, 0.8429, 0.9065, 1.0000]], grad_fn=<MeanBackward1>)\n",
      "covariance std: tensor([[0.0000, 0.0028, 0.0431, 0.1790],\n",
      "        [0.0028, 0.0000, 0.0431, 0.1577],\n",
      "        [0.0431, 0.0431, 0.0000, 0.1215],\n",
      "        [0.1790, 0.1577, 0.1215, 0.0000]], grad_fn=<StdBackward1>)\n"
     ]
    }
   ],
   "source": [
    "arm_covariances = get_arm_covariance(30, 10)\n",
    "\n",
    "print(f\"mean covariance: {arm_covariances.mean(dim=0)}\")\n",
    "print(f\"covariance std: {arm_covariances.std(dim=0)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Multiple fit tries does help reduce the variability, but it still does not eliminate it.\n",
    "\n",
    "With Adam"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean covariance: tensor([[1.0000, 0.9625, 0.9384, 0.8415],\n",
      "        [0.9625, 1.0000, 0.9644, 0.8578],\n",
      "        [0.9384, 0.9644, 1.0000, 0.9506],\n",
      "        [0.8415, 0.8578, 0.9506, 1.0000]], grad_fn=<MeanBackward1>)\n",
      "covariance std: tensor([[0.0000, 0.1747, 0.1362, 0.1227],\n",
      "        [0.1747, 0.0000, 0.0606, 0.1326],\n",
      "        [0.1362, 0.0606, 0.0000, 0.0425],\n",
      "        [0.1227, 0.1326, 0.0425, 0.0000]], grad_fn=<StdBackward1>)\n"
     ]
    }
   ],
   "source": [
    "arm_covariances = get_arm_covariance(30, 10, True)\n",
    "\n",
    "print(f\"mean covariance: {arm_covariances.mean(dim=0)}\")\n",
    "print(f\"covariance std: {arm_covariances.std(dim=0)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That was a long shot, and the results are as expected. The optimizer is not the problem\n",
    ". We just have too many local optima, so the results really depend on the initialization.\n",
    "\n",
    "## One last thing we could try is to initialize the embedding as a prior draw?\n",
    "\n",
    "So, HOGP uses either `torch.rand` or draws from prior to initialize latents. We let\n",
    "`Embedding` handle the initialization, and use `torch.randn` when we do multiple fit\n",
    "tries. Let's give their methods a shot."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean covariance: tensor([[1.0000, 0.9669, 0.8718, 0.6477],\n",
      "        [0.9669, 1.0000, 0.8624, 0.6644],\n",
      "        [0.8718, 0.8624, 1.0000, 0.8314],\n",
      "        [0.6477, 0.6644, 0.8314, 1.0000]], grad_fn=<MeanBackward1>)\n",
      "covariance std: tensor([[0.0000, 0.1403, 0.1953, 0.2896],\n",
      "        [0.1403, 0.0000, 0.2349, 0.2862],\n",
      "        [0.1953, 0.2349, 0.0000, 0.2118],\n",
      "        [0.2896, 0.2862, 0.2118, 0.0000]], grad_fn=<StdBackward1>)\n"
     ]
    }
   ],
   "source": [
    "arm_covariances = get_arm_covariance(30, 1, False, \"rand\")\n",
    "\n",
    "print(f\"mean covariance: {arm_covariances.mean(dim=0)}\")\n",
    "print(f\"covariance std: {arm_covariances.std(dim=0)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean covariance: tensor([[1.0000, 0.9617, 0.9107, 0.6707],\n",
      "        [0.9617, 1.0000, 0.9178, 0.7139],\n",
      "        [0.9107, 0.9178, 1.0000, 0.8982],\n",
      "        [0.6707, 0.7139, 0.8982, 1.0000]], grad_fn=<MeanBackward1>)\n",
      "covariance std: tensor([[0.0000, 0.1776, 0.0167, 0.0299],\n",
      "        [0.1776, 0.0000, 0.1615, 0.1043],\n",
      "        [0.0167, 0.1615, 0.0000, 0.0130],\n",
      "        [0.0299, 0.1043, 0.0130, 0.0000]], grad_fn=<StdBackward1>)\n"
     ]
    }
   ],
   "source": [
    "arm_covariances = get_arm_covariance(30, 1, True, \"rand\")\n",
    "\n",
    "print(f\"mean covariance: {arm_covariances.mean(dim=0)}\")\n",
    "print(f\"covariance std: {arm_covariances.std(dim=0)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This one is interesting. `rand` and `Adam` coupled together yield significantly less\n",
    "variance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean covariance: tensor([[1.0000, 0.9926, 0.9423, 0.7525],\n",
      "        [0.9926, 1.0000, 0.9497, 0.7720],\n",
      "        [0.9423, 0.9497, 1.0000, 0.8999],\n",
      "        [0.7525, 0.7720, 0.8999, 1.0000]], grad_fn=<MeanBackward1>)\n",
      "covariance std: tensor([[0.0000, 0.0191, 0.0484, 0.2159],\n",
      "        [0.0191, 0.0000, 0.0786, 0.2100],\n",
      "        [0.0484, 0.0786, 0.0000, 0.1208],\n",
      "        [0.2159, 0.2100, 0.1208, 0.0000]], grad_fn=<StdBackward1>)\n"
     ]
    }
   ],
   "source": [
    "arm_covariances = get_arm_covariance(30, 1, False, \"gp\")\n",
    "\n",
    "print(f\"mean covariance: {arm_covariances.mean(dim=0)}\")\n",
    "print(f\"covariance std: {arm_covariances.std(dim=0)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean covariance: tensor([[1.0000, 0.9937, 0.9212, 0.6869],\n",
      "        [0.9937, 1.0000, 0.9554, 0.7477],\n",
      "        [0.9212, 0.9554, 1.0000, 0.8973],\n",
      "        [0.6869, 0.7477, 0.8973, 1.0000]], grad_fn=<MeanBackward1>)\n",
      "covariance std: tensor([[0.0000, 0.0042, 0.0253, 0.0454],\n",
      "        [0.0042, 0.0000, 0.0177, 0.0347],\n",
      "        [0.0253, 0.0177, 0.0000, 0.0152],\n",
      "        [0.0454, 0.0347, 0.0152, 0.0000]], grad_fn=<StdBackward1>)\n"
     ]
    }
   ],
   "source": [
    "arm_covariances = get_arm_covariance(30, 1, True, \"gp\")\n",
    "\n",
    "print(f\"mean covariance: {arm_covariances.mean(dim=0)}\")\n",
    "print(f\"covariance std: {arm_covariances.std(dim=0)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, using `Adam` with `rand` or `gp` does reduce the variability. Let's give it a shot\n",
    "and see how it goes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}